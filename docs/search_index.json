[
["index.html", "Road to Prediction Prerequiste", " Road to Prediction Sebastian Nickel October 25, 2018 Prerequiste This project’s goal is to predict when highways were built with the help of NASA Landsat satellite images and machine learning techniques. Everything here is work in progress. "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction I decided to test the idea of road prediction on the North-Eastern German state of Mecklenburg-Vorpommern (MV). MV is one of the “new” German states and former territory of the German Democratic Republic (GDR). In the 90s and 2000s after the former GDR territories joined the Federal Republic of Germany a lot of development projects were carried out in the new states to promote “flourishing landscapes”. Most notably in MV the motorway A20 (so-called Baltic Sea motorway) was built. I only rely on Landsat images of Landsat 4 and higher because the resolution on Landsat 1-3 is not high enough. Landsat 4 started in 1982 and the resolution for 5 of the 7 band is 30 square meters at the equator. "],
["extracting-road-data.html", "Chapter 2 Extracting road data", " Chapter 2 Extracting road data The road data for MV is extracted via the overpass API, the necessary overpass package is provided by hrbrmstr. I extract two types of roads, first the highways and then all primary roads. Especially, the second query is time consuming. library(overpass) library(rgdal) library(tidyverse) library(raster) library(sf) library(xgboost) library(keras) library(randomForest) library(data.table) library(geosampling) library(rgeos) library(knitr) library(magrittr) options(stringsAsFactors = FALSE) ## Reading layer `gadm36_DEU_1&#39; from data source `C:\\Users\\xnicse\\Dropbox\\Projects\\road_to_prediction\\data\\spatial\\gadm\\gadm36_DEU_1.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 16 features and 10 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 5.866251 ymin: 47.27012 xmax: 15.04181 ymax: 55.05653 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs Figure 2.1: Mecklenburg-Vorpommern and bounding box in red. motorway_query &lt;- &#39;[out:xml][timeout:100]; ( node[&quot;highway&quot;=&quot;motorway&quot;](53,10.5,54.7,14.5); way[&quot;highway&quot;=&quot;motorway&quot;](53,10.5,54.7,14.5); relation[&quot;highway&quot;=&quot;motorway&quot;](53,10.5,54.7,14.5); ); out body; &gt;; out skel qt;&#39; motorway_spatial &lt;- overpass_query(motorway_query) primary_query &lt;- &#39;[out:xml][timeout:100]; ( node[&quot;highway&quot;=&quot;primary&quot;](53,10.5,54.7,14.5); way[&quot;highway&quot;=&quot;primary&quot;](53,10.5,54.7,14.5); relation[&quot;highway&quot;=&quot;primary&quot;](53,10.5,54.7,14.5); ); out body; &gt;; out skel qt;&#39; primary_spatial &lt;- overpass_query(primary_query) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Users\\xnicse\\Dropbox\\Projects\\road_to_prediction\\output\\roads_in_mv\\motorway.shp&quot;, layer: &quot;motorway&quot; ## with 2355 features ## It has 81 fields ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Users\\xnicse\\Dropbox\\Projects\\road_to_prediction\\output\\roads_in_mv\\primary.shp&quot;, layer: &quot;primary&quot; ## with 9003 features ## It has 271 fields Figure 2.2: MV with motorways (red) and primary roads (blue). "],
["selecting-landsat-images.html", "Chapter 3 Selecting Landsat images 3.1 Locating the scenes 3.2 Reducing the scenes 3.3 Bulk download scenes", " Chapter 3 Selecting Landsat images Unfortunately, the NASA Landsat images need to be selected via an online mask on USGS’s webpage. My approach was to extract the identifier of the images that I want to download and then feed them into the USGS website. This way a bulk download is possible. The necessary file to identify the images can be downloaded from USGS. I only care about pictures taken between June and August by Landsat 5 which operated between 1985 and 2011. The reason is that the sizes of the images are large, including scences from the whole year means that the reflected colors change and thus, more images are necessary to train the algorithm. Additionally, clouds block the view and I assume that there are less clouds in the summer than in the winter, which increases the number of useable scenes. 3.1 Locating the scenes First, I identify the PATH and the ROW which are necessery to identify the scenes. wrs2&lt;-readOGR(&quot;data/spatial/wrs2/wrs2_descending.shp&quot;, verbose=FALSE) pol_extent2 &lt;- as(pol_extent,&quot;Spatial&quot;) %&gt;% SpatialPolygonsDataFrame(data=data.frame(x=1)) mv_wrs2 &lt;- wrs2[which(!is.na(over(wrs2,pol_extent2,returnList =FALSE))),] mv_wrs2 %&gt;% st_as_sf() %&gt;% ggplot() + geom_sf(data=mv) + geom_sf(fill=NA) Figure 3.1: MV with scenes. With the PATH and ROW numbers (Table 3.1), I turned to the eartexplorer website to search for the images. Instead of downloading the images, which are around 2 GB each, I extracted the meta file (Figure 3.2, 3.3, and 3.4). scenes &lt;- wrs2@data[which(!is.na(over(wrs2,pol_extent2,returnList =FALSE))),c(&quot;PATH&quot;,&quot;ROW&quot;)] data.frame(PATH = c(min(scenes$PATH),max(scenes$PATH)), ROW = c(min(scenes$ROW),max(scenes$ROW)),row.names = c(&quot;Min&quot;,&quot;Max&quot;)) %&gt;% kable(cap=&quot;PATH and ROW to retrieve scene list from Earthexplorer.&quot;) Table 3.1: PATH and ROW to retrieve scene list from Earthexplorer. PATH ROW Min 192 22 Max 196 23 Figure 3.2: Screenshot of selecting Landsat 4-5 from Earthexplorer. Figure 3.3: Screenshot of selecting Landsat 4-5 from Earthexplorer. Figure 3.4: Screenshot of selecting Landsat 4-5 from Earthexplorer. 3.2 Reducing the scenes The metadata can be read it and analyzed on its own. First, I only select images from Landsat 5 between June and August. I then calculate the lowest cloud coverage for each scene in each year. Third, I calculate the overal maximum of the lowest yearly coverages and the mean cloud coverare for each scene (Table 3.2). It turns out that scene 194@22 and 194@23 have the most favorable numbers and cover most of the territory of MV (Figure 3.5) images_df &lt;- read.csv(&quot;data/rectangular/scene_list/LSR_LANDSAT_TM_C1_279839.csv&quot;) %&gt;% mutate(month = strsplit(Acquisition.Date,&quot;/&quot;) %&gt;% lapply(`[[`,2) %&gt;% unlist() %&gt;% as.numeric(), year = strsplit(Acquisition.Date,&quot;/&quot;) %&gt;% lapply(`[[`,1) %&gt;% unlist() %&gt;% as.numeric(), satelitte = strsplit(Landsat.Product.Identifier,&quot;_&quot;) %&gt;% lapply(`[[`,1) %&gt;% unlist(), PATHROW = strsplit(Landsat.Product.Identifier,&quot;_&quot;) %&gt;% lapply(`[[`,3) %&gt;% unlist()) %&gt;% mutate(PATH = substr(PATHROW,1,3) %&gt;% as.numeric(), ROW = substr(PATHROW,4,6) %&gt;% as.numeric()) %&gt;% filter(month &gt; 6 &amp; month &lt; 9) %&gt;% filter(satelitte == &quot;LT05&quot;) exp &lt;- images_df %&gt;% expand(year,PATH,ROW) exp$minimum_cloud_cover &lt;- apply(exp,1,function(x) { f1 &lt;- images_df %&gt;% filter(year == x[1]&amp; PATH == x[2]&amp; ROW == x[3]) if (nrow(f1) == 0) return(NA) f1 %$% Land.Cloud.Cover %&gt;% min }) exp %&gt;% mutate(PATHROW=paste0(PATH,&quot;@&quot;,ROW)) %&gt;% group_by(PATHROW) %&gt;% summarise(`Maximum of yearly minimum cloud cover`=max(minimum_cloud_cover,na.rm=TRUE), `Mean Cloud Cover`=round(mean(minimum_cloud_cover,na.rm=TRUE),2)) %&gt;% kable(caption = &quot;Maximum and mean cloud cover for different scenes.&quot;) Table 3.2: Maximum and mean cloud cover for different scenes. PATHROW Maximum of yearly minimum cloud cover Mean Cloud Cover 192@22 97 19.22 192@23 69 17.44 193@22 100 19.74 193@23 91 23.67 194@22 74 15.31 194@23 66 14.30 195@22 77 24.07 195@23 55 21.67 196@22 100 27.19 196@23 70 29.96 ggplot() + geom_sf(data = mv) + geom_sf(data = mv_wrs2 %&gt;% st_as_sf() %&gt;% mutate(PATHROW=paste0(PATH,&quot;@&quot;,ROW)) %&gt;% filter(PATHROW %in% c(&quot;194@22&quot;,&quot;194@23&quot;)),fill=NA) Figure 3.5: Scenes 194@22 and 194@23. cloudthreshold &lt;- 50 number_of_scenes &lt;- images_df %&gt;% mutate(PATHROW=paste0(PATH,&quot;@&quot;,ROW)) %&gt;% filter(PATHROW %in% c(&quot;194@22&quot;,&quot;194@23&quot;)&amp;Land.Cloud.Cover&lt;cloudthreshold&amp;grepl(&quot;_T1&quot;,Landsat.Product.Identifier)) %&gt;% nrow I further reduce the number of scenes to only account for images with a cover less than 50%. A total of 79 are still being considered. Figure 3.6 shows the distribution of cloud covers for both scenes from 1984 to 2011. images_df %&gt;% mutate(PATHROW=paste0(PATH,&quot;@&quot;,ROW)) %&gt;% filter(PATHROW %in% c(&quot;194@22&quot;,&quot;194@23&quot;)&amp;Land.Cloud.Cover&lt;cloudthreshold&amp;grepl(&quot;_T1&quot;,Landsat.Product.Identifier)) %&gt;% ggplot() + geom_point(aes(x=year,y=Land.Cloud.Cover,group=PATHROW,color=PATHROW)) + ylab(&quot;Area covered by clouds&quot;) + xlab(&quot;Year&quot;) + scale_color_discrete(name=&quot;Scenes&quot;) Figure 3.6: Scenes 3.3 Bulk download scenes images_df %&gt;% mutate(PATHROW=paste0(PATH,&quot;@&quot;,ROW)) %&gt;% filter(PATHROW %in% c(&quot;194@22&quot;,&quot;194@23&quot;)&amp;Land.Cloud.Cover&lt;cloudthreshold&amp;grepl(&quot;_T1&quot;,Landsat.Product.Identifier)) %&gt;% dplyr::select(Landsat.Product.Identifier) %&gt;% fwrite(&quot;scene_list.csv&quot;,col.names = FALSE) The list needs to be uploaded on Earthexplorer. Figure 3.7: Screenshot of Earthexplorer mask. Once the scenes are ready, I copied the source code of the website and extracted the urls to the files since they are not directly password protected. Then I downloaded them and untarred them. website &lt;- readLines(&quot;data/landsatwebsite/website&quot;,warn=FALSE) urls &lt;- website[grep(&quot;Download&lt;/a&gt;&quot;,website)] %&gt;% strsplit(&quot;&gt;&quot;) %&gt;% lapply(`[[`,1) %&gt;% unlist() %&gt;% gsub(&quot;&lt;a href=\\&quot;&quot;,&quot;&quot;,.) %&gt;% gsub(&quot;\\&quot;&quot;,&quot;&quot;,.) %&gt;% gsub(&quot; &quot;,&quot;&quot;,.) save_files &lt;- urls %&gt;% strsplit(&quot;/&quot;) %&gt;% lapply(`[[`,6) %&gt;% unlist() sapply(1:length(urls),function(x) { download.file(url = urls[x], destfile = paste0(&quot;E:/landsatscene/&quot;,save_files[x])) }) tar_files &lt;- list.files(&quot;E:/landsatscene/&quot;,full.names = TRUE) sapply(tar_files,function(x) { if (file.exists(x %&gt;% gsub(&quot;.tar.gz&quot;,&quot;&quot;,.))) return() untar(x,exdir = x %&gt;% gsub(&quot;.tar.gz&quot;,&quot;&quot;,.)) }) "],
["preparation-for-a-basic-analysis.html", "Chapter 4 Preparation for a basic analysis", " Chapter 4 Preparation for a basic analysis First, I will look at one satellite picture to show the general procedure. According to the Landsat website the pixel_qa band has a more accurate prediction for identifying clouds, cloud shadows, water, and snow. When transforming spatial vector objects to raster objects, I always rely on the fasterize package which is written in C++ and a lot faster than the functions in the raster package. cloud_identification &lt;- raster(&quot;E:/landsatscene/LT051940232011072701T1-SC20180926185810/LT05_L1TP_194023_20110727_20180312_01_T1_pixel_qa.tif&quot;) # crop raster to extent of MV cloud_identification_cropped &lt;- crop(cloud_identification, st_transform(mv,crs=proj4string(cloud_identification))) # transform MV Polygon to a raster mv_raster &lt;- fasterize::fasterize(st_transform(mv, crs=proj4string(cloud_identification)), cloud_identification_cropped) # set all values that lie outside of MV to NA cloud_identification_masked &lt;- mask(cloud_identification_cropped,mv_raster) # extract the values of the cloud raster cloud_identification_values &lt;- getValues(cloud_identification_masked) # reference values for not having a clear sight on the ground none_clear_values &lt;- c(96, 112,160,176,224,68,72,80,96, 112,132,136,144,160,176,224,72, 136,68,132,80,112,144,176) # Create a mask vector. NA for each pixel outside of MV, 0 for if its inside # but not a clear view, and 1 if its inside MV and there is a clear view cloud_identification_values_2 &lt;- ifelse(cloud_identification_values %in% none_clear_values,0, ifelse(is.na(cloud_identification_values),NA,1)) # create raster from mask vector cloud_identification_2 &lt;- setValues(cloud_identification_masked, cloud_identification_values_2) # transform motorways to a SpatialPolygon by buffering 1 meter around the lines motorways_sf &lt;- spTransform(motorways, CRS(proj4string(cloud_identification_2))) %&gt;% gBuffer(width = 2) %&gt;% st_as_sf() # transform roads to a raster motorways_raster &lt;- fasterize::fasterize(motorways_sf, cloud_identification_2) covered_by_road &lt;- mask(cloud_identification_2,motorways_raster) %&gt;% getValues() %&gt;% is.na() %&gt;% `!` %&gt;% which() cloud_identification_values_3 &lt;- ifelse(is.na(cloud_identification_values_2),0,cloud_identification_values_2) Table 4.1: Coverage for different values. NA are the rims of the picture, 0 is cloud covered, and 1 is free sight. Percentage Values 28.97 0 28.17 1 42.86 NA The following code chunk loads each band, crops it to the extent of MV, and sets all pixel to NA that either have no undisturbed sight or lie outside of MV. The values are bound together in a rectengular data.frame with the added information whether the specific pixel is a motorway or not. All NAs are subsequently removed from the data.frame and all values are mean normalized. listed_raster &lt;- list.files(&quot;E:/landsatscene/LT051940232011072701T1-SC20180926185810/&quot;,pattern=&quot;band[0-9].tif$&quot;,full.names = TRUE) values_df &lt;- lapply(listed_raster,function(ra1_path) { ra1 &lt;- raster(ra1_path) ra1_cropped &lt;- crop(ra1,st_transform(mv,crs=proj4string(ra1))) ra1_cropped_masked &lt;- mask(ra1_cropped,mv_raster) ra1_cropped_masked_cloudless &lt;- setValues(ra1_cropped_masked,ifelse(getValues(ra1_cropped_masked)*cloud_identification_values_3==0,NA,getValues(ra1_cropped_masked))) getValues(ra1_cropped_masked_cloudless) }) %&gt;% do.call(cbind,.) %&gt;% as.data.frame() values_df$motorway &lt;- 0 values_df$motorway[covered_by_road] &lt;- 1 identify_na &lt;- sapply(paste0(&quot;V&quot;,1:6),function(x) { values_df[,x] %&gt;% is.na() %&gt;% which() }) %&gt;% unlist() %&gt;% unique() if (length(identify_na) &gt; 0) { values_df &lt;- values_df %&gt;% dplyr::slice(-identify_na) } norm_mean &lt;- function(x) { (x-mean(x))/(max(x)-min(x)) } values_df[,paste0(&quot;V&quot;,1:6)] &lt;- values_df[,paste0(&quot;V&quot;,1:6)] %&gt;% apply(2,norm_mean) "],
["basic-analysis.html", "Chapter 5 Basic Analysis 5.1 Ordinary Least Squares 5.2 Logit 5.3 Random Forest 5.4 XGBoost 5.5 Neural Network 5.6 Convolutional Neural Network", " Chapter 5 Basic Analysis I am using OLS, Random Forest, XGBoost, a neural network, and a convolutional neural network to predict motorway pixel. The train/test split is for all methods 80/20 and I run each method 100 times to get to different splits. The results are presented in the next chapter. The train_and_test_data function is creating a train and test set. If replace_motorway is set to TRUE the same amount of motorway pixel will be sampled for the training data as of no motorway pixel. This way, the distribution will train_and_test_data &lt;- function(seed,df,trainsize=0.8, replace_motorway=TRUE, sample_size=1e4, sample_test=1e4, factorize_dependent=FALSE) { set.seed(seed) if (factorize_dependent) df$motorway &lt;- as.factor(df$motorway) motorway_pixel &lt;- which(df$motorway==1) no_motorway_pixel &lt;- which(df$motorway==0) sampled_motorway_pixel &lt;- sample(motorway_pixel, round(trainsize*length(motorway_pixel))) sampled_no_motorway_pixel &lt;- sample(no_motorway_pixel, round(trainsize*length(no_motorway_pixel))) test_motorway_pixel &lt;- motorway_pixel[!motorway_pixel %in% sampled_motorway_pixel] test_no_motorway_pixel &lt;- no_motorway_pixel[!no_motorway_pixel %in% sampled_no_motorway_pixel] values_df_train &lt;- df[c(sampled_motorway_pixel,sampled_no_motorway_pixel),] values_df_test &lt;- df[c(test_motorway_pixel,test_no_motorway_pixel),] # train if (replace_motorway) { selected_motorway &lt;- sample(sampled_motorway_pixel,sample_size,replace=TRUE) selected_non_motorway &lt;- sample(sampled_no_motorway_pixel,sample_size) } else { selected_motorway &lt;- sampled_motorway_pixel selected_non_motorway &lt;- sample(sampled_no_motorway_pixel,sample_size) } subset &lt;- c(selected_non_motorway,selected_motorway) values_df_model &lt;- dplyr::slice(df,subset) # test subset_test &lt;- c(test_motorway_pixel,sample(test_no_motorway_pixel, sample_test)) values_df_test &lt;- dplyr::slice(df,subset_test) list(train=values_df_model, test=values_df_test) } predicted &lt;- function(data) { data %&gt;% dplyr::rename(Type=motorway) %&gt;% group_by(Type) %&gt;% dplyr::summarise(Value = sum(equal)/n()) %&gt;% mutate(Type=ifelse(Type==1,&quot;Motorway&quot;,&quot;None Motorway&quot;)) } set.seed(15) random_number &lt;- sample(1e5,100) 5.1 Ordinary Least Squares generated &lt;- lapply(random_number,function(x) { train_test_list &lt;- train_and_test_data(seed = x, df = values_df, trainsize = 0.8, sample_size = 1e4, sample_test = 1e4) values_df_model &lt;- train_test_list$train values_df_test &lt;- train_test_list$test # model model_linear &lt;- lm(motorway~V1+V2+V3+V4+V5+V6,data=values_df_model) # predict values_df_test$predicted &lt;- predict(model_linear,values_df_test) values_df_test &lt;- values_df_test %&gt;% mutate(predicted_binary = as.numeric(predicted&gt;0.5), equal = as.numeric(motorway==predicted_binary)) values_df_test %&gt;% predicted() }) %&gt;% do.call(rbind,.) 5.2 Logit generated_glm &lt;- lapply(random_number,function(x) { train_test_list &lt;- train_and_test_data(seed = x, df = values_df, trainsize = 0.8, sample_size = 1e4, sample_test = 1e4) values_df_model &lt;- train_test_list$train values_df_test &lt;- train_test_list$test # model model &lt;- glm(motorway~V1+V2+V3+V4+V5+V6,data=values_df_model,family=binomial) # predict values_df_test$predicted &lt;- predict(model,values_df_test) values_df_test &lt;- values_df_test %&gt;% mutate(predicted_binary = as.numeric(predicted&gt;0.5), equal = as.numeric(motorway==predicted_binary)) values_df_test %&gt;% predicted() }) %&gt;% do.call(rbind,.) 5.3 Random Forest generated_rf &lt;- lapply(random_number,function(x) { values_df$motorway &lt;- as.factor(values_df$motorway) train_test_list &lt;- train_and_test_data(seed = x, df = values_df, trainsize = 0.8, replace_motorway = FALSE, sample_size = 1e3, sample_test = 1e4) values_df_model &lt;- train_test_list$train values_df_test &lt;- train_test_list$test # train model_rf&lt;-randomForest( formula = motorway~V1+V2+V3+V4+V5+V6, data= values_df_model, ntree=500, importance=TRUE)#, # predict values_df_test$predicted &lt;- predict(model_rf,values_df_test %&gt;% dplyr::select(paste0(&quot;V&quot;,1:6),motorway)) values_df_test2 &lt;- values_df_test %&gt;% mutate(equal = as.numeric(motorway==predicted)) values_df_test2 %&gt;% predicted() }) %&gt;% do.call(rbind,.) 5.4 XGBoost generated_xgb &lt;- lapply(random_number,function(x) { train_test_list &lt;- train_and_test_data(seed = x, df = values_df, trainsize = 0.8, replace_motorway = FALSE, sample_size = 1e3, sample_test = 1e4) values_df_model &lt;- train_test_list$train values_df_test &lt;- train_test_list$test train_dat&lt;-xgb.DMatrix( data = values_df_model %&gt;% dplyr::select(paste0(&quot;V&quot;,1:6)) %&gt;% as.matrix(), label = values_df_model %&gt;% dplyr::select(motorway) %&gt;% unlist) model_xgboost&lt;-xgboost(data = train_dat, # the data nrounds=1000, eta=0.025, subsample=0.63, lamba=0.001, objective = &quot;binary:logistic&quot;) test_dat&lt;-xgb.DMatrix( data = values_df_test %&gt;% dplyr::select(paste0(&quot;V&quot;,1:6)) %&gt;% as.matrix(), label = values_df_test %&gt;% dplyr::select(motorway) %&gt;% unlist) values_df_test$predicted &lt;- predict(model_xgboost,test_dat) values_df_test2 &lt;- values_df_test %&gt;% mutate(predicted_binary = as.numeric(predicted&gt;0.5), equal = as.numeric(motorway==predicted_binary)) values_df_test2 %&gt;% predicted() }) %&gt;% do.call(rbind,.) 5.5 Neural Network generated_keras &lt;- lapply(random_number,function(x) { train_test_list &lt;- train_and_test_data(seed = x, df = values_df, trainsize = 0.8, replace_motorway = TRUE, sample_size = 2e4, sample_test = 1e4) values_df_model &lt;- train_test_list$train values_df_test &lt;- train_test_list$test model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 256, activation = &#39;elu&#39;, input_shape = c(6),kernel_regularizer = regularizer_l2(0.001)) %&gt;% layer_dense(units = 128, activation = &#39;elu&#39;, input_shape = c(6),kernel_regularizer = regularizer_l2(0.001)) %&gt;% layer_dense(units = 64, activation = &#39;elu&#39;, input_shape = c(6),kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_dense(units = 32, activation = &#39;elu&#39;, input_shape = c(6),kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_dense(units = 16,activation = &#39;elu&#39;, input_shape = c(6),kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_dense(units = 8, activation = &#39;elu&#39;, kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_dense(units = 4, activation = &#39;elu&#39;, kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_dense(units = 3, activation = &#39;elu&#39;, kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_dense(units = 2, activation = &#39;sigmoid&#39;) model %&gt;% compile( loss = &#39;binary_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = c(&#39;accuracy&#39;)) k_set_value(model$optimizer$lr, 0.00001) k_set_value(model$optimizer$decay, 0.00001/100) history &lt;- model %&gt;% fit( x = values_df_model %&gt;% dplyr::select(paste0(&quot;V&quot;,1:6)) %&gt;% apply(2,as.numeric), y = values_df_model %&gt;% dplyr::select(motorway) %&gt;% apply(2,as.numeric)%&gt;% to_categorical(), epochs = 60, batch_size = 30, validation_split = 0.1) predicted &lt;- model %&gt;% predict(values_df_test %&gt;% dplyr::select(paste0(&quot;V&quot;,1:6)) %&gt;% apply(2,as.numeric)) values_df_test$predicted &lt;- predicted[,2] values_df_test2 &lt;- values_df_test %&gt;% mutate(predicted_binary = as.numeric(predicted&gt;0.5), equal = as.numeric(motorway==predicted_binary)) values_df_test2 %&gt;% predicted() }) %&gt;% do.call(rbind,.) 5.6 Convolutional Neural Network listed_raster &lt;- list.files(&quot;E:/landsatscene/LT051940232011072701T1-SC20180926185810/&quot;,pattern=&quot;band[0-9].tif$&quot;,full.names = TRUE) norm_mean &lt;- function(x) { (x-mean(x,na.rm=TRUE))/(max(x,na.rm=TRUE)-min(x,na.rm=TRUE)) } ra1_path &lt;- listed_raster[1] brick &lt;- lapply(listed_raster,function(ra1_path) { # ra1 &lt;- raster(ra1_path) ve1 &lt;- velox(ra1_path) ve1$crop(mv_st %&gt;% extent %&gt;% as.vector()) ra1_cropped &lt;- ve1$as.RasterLayer() ra1_cropped_masked &lt;- mask(ra1_cropped,mv_raster) ra1_cropped_masked_values &lt;- getValues(ra1_cropped_masked) cloudless_values &lt;- ra1_cropped_masked_values *cloud_identification_values_3 ra1_cropped_masked_cloudless_values &lt;- ifelse_cpp(cloudless_values,0,0,NA,ra1_cropped_masked_values) ra1_cropped_masked_cloudless_values_norm &lt;- norm_mean(ra1_cropped_masked_cloudless_values) setValues(ra1_cropped_masked,ra1_cropped_masked_cloudless_values_norm) }) %&gt;% brick # aggregate # to polygon # extract from each band # return as array in list # aggregate raster to get mask to crop -- faster with velox b1_velox &lt;- velox(brick[[1]]) b1_agg &lt;- b1_velox$copy() b1_agg$aggregate(factor=c(5,5),aggtype=&quot;sum&quot;) agg_raster &lt;- b1_agg$as.RasterLayer() agg_raster %&gt;% getValues() %&gt;% is.na %&gt;% which %&gt;% length agg_raster %&gt;% getValues() %&gt;% length # transform mask to spatialpolygon b1_agg_poly &lt;- agg_raster %&gt;% as(&quot;SpatialPolygons&quot;) # again, do it in velox because its way faster brick_velox &lt;- velox(brick) array_of_images &lt;- laply(1:length(b1_agg_poly),function(y) { if (y%%1000==0) message(y) b1 &lt;- brick_velox$copy() b1$crop(b1_agg_poly[y,]) laply(1:6,function(x) b1$as.matrix(x)) }) # remove with NA del &lt;- lapply(1:dim(array_of_images)[1],function(x) array_of_images[x,,,] %&gt;% is.na %&gt;% any) %&gt;% unlist %&gt;% which array_of_images &lt;- array_of_images[-del,,,] # for Yd motorways_raster01 &lt;- setValues(motorways_raster,ifelse(is.na(getValues(motorways_raster)),0,1)) motorways_velox &lt;- velox(motorways_raster01) motorways_velox$aggregate(factor=c(5,5),aggtype=&quot;max&quot;) Y &lt;- motorways_velox$extract(b1_agg_poly,fun=max) Y &lt;- Y[-del,] Y_cat &lt;- to_categorical(Y %&gt;% as.vector()) pos &lt;- which(Y==1) generated_cnn &lt;- lapply(random_number,function(x) { set.seed(x) pos_train &lt;- sample(pos,round(length(pos)*0.8)) %&gt;% rep(8) pos_test &lt;- pos[!pos%in%pos_train] %&gt;% rep(8) neg &lt;- which(Y==0) neg_train &lt;- sample(neg,length(pos_train)) neg_test &lt;- sample(neg[!neg%in%neg_train],length(pos_test)) y_train &lt;- Y_cat[c(pos_train,neg_train) %&gt;% sort,] # y_val &lt;- Y_cat[c(pos_val,neg_val) %&gt;% # sort,] y_test &lt;- Y_cat[c(pos_test,neg_test) %&gt;% sort,] x_train &lt;- array_of_images[c(pos_train,neg_train) %&gt;% sort,,,] # x_val &lt;- array_of_images[c(pos_val,neg_val) %&gt;% # sort,,,] x_test &lt;- array_of_images[c(pos_test,neg_test) %&gt;% sort,,,] model&lt;-keras_model_sequential() model %&gt;% layer_conv_2d(filters=5,kernel_size=5,data_format=&quot;channels_first&quot;, padding=&quot;same&quot;,input_shape=c(6,5,5),activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.01) ) %&gt;% layer_conv_2d(filter=10,kernel_size=5,activation=&quot;elu&quot;,padding=&quot;same&quot;, kernel_regularizer = regularizer_l2(0.0001)) %&gt;% layer_max_pooling_2d(pool_size=c(3,3)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 64,activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.0001) ) %&gt;% layer_dense(units = 32,activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.0001) ) %&gt;% layer_dense(units = 16,activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.0001) ) %&gt;% layer_dense(units = 8,activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.0001) ) %&gt;% layer_dense(units = 4,activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.0001) ) %&gt;% layer_dense(units = 3,activation = &quot;elu&quot; ,kernel_regularizer = regularizer_l2(0.0001) ) %&gt;% layer_dense(2,activation = &quot;sigmoid&quot;) model %&gt;% compile(loss=&quot;categorical_crossentropy&quot;, optimizer=optimizer_rmsprop(lr = 0.0001,decay = 1e-8), # optimizer_adam( lr= 0.0001 , decay = 1e-8 ), metrics = &quot;accuracy&quot;) model %&gt;% fit( x_train,y_train ,batch_size=32, epochs=60,#validation_data = list(x_val, y_val), shuffle=TRUE) predicted &lt;- model %&gt;% predict(x_test) pred &lt;- data.frame(orig=y_test[,2],pred=ifelse(predicted[,2]&gt;0.5,1,0)) %&gt;% mutate(res=apply(.,1,function(x) { if (x[1]==x[2]) return(1) 0 })) %&gt;% group_by(orig,res) %&gt;% dplyr::summarize(n=dplyr::n()) data.frame(Type=c(&quot;None Motorway&quot;,&quot;Motorway&quot;), Value = c(pred$n[2]/sum(pred$n[1:2]), pred$n[4]/sum(pred$n[3:4]))) }) %&gt;% do.call(rbind,.) "],
["evaluation-of-basic-models.html", "Chapter 6 Evaluation of Basic Models", " Chapter 6 Evaluation of Basic Models all_simulations &lt;- list(OLS=generated,logit=generated_glm, RandomForest=generated_rf,XGBoost=generated_xgb, Keras=generated_keras,CNN=generated_cnn) Before evaluating, I remove one prediction of the neural network that predicts that all pixels are none road pixels. all_simulations_df &lt;- names(all_simulations) %&gt;% lapply(function(name) { dat &lt;- all_simulations[[name]] dat$Method &lt;- name dat %&gt;% filter(!is.na(Value)) }) %&gt;% do.call(rbind,.) %&gt;% filter(Value&gt;0&amp;Value&lt;1) nrow(all_simulations_df)/2 ## [1] 598 Figure 6.1 shows the accuracy for six different methods. All approaches yield similar results with the average accuracy for motorway prediction lying between 68% and 90%. The accuracy of predicting the none motorway pixels is slightly higher and the difference between the methods is more obvious. In general OLS performs less well than the other three and the neural network has the greates span and even has some predictions below 70% for motorway. Random Forest and XGBoost yield the best results, especially when it comes to the “none motorway” prediction were both are consistently above 90%. all_simulations_df %&gt;% group_by(Method,Type) %&gt;% dplyr::summarise(mean=round(mean(Value),4)*100) %&gt;% spread(Type,mean) %&gt;% kable(caption = &quot;Accuracy for motorway and none motorway pixels for different methods in percent.&quot;) Table 6.1: Accuracy for motorway and none motorway pixels for different methods in percent. Method Motorway None Motorway CNN 67.00 87.34 Keras 78.08 84.51 logit 71.21 89.59 OLS 78.31 84.15 RandomForest 80.44 92.07 XGBoost 81.67 91.50 all_simulations_df %&gt;% ggplot() + geom_violin(aes(x=Type,y=Value,fill=as_factor(Method))) + ylab(&quot;Accuracy&quot;) + guides(fill=guide_legend(title=&quot;Methods&quot;)) Figure 6.1: Comparison between OLS, Random Forest, XGBoost, and neural network (Keras). "],
["open-questionsissues.html", "Chapter 7 Open Questions/Issues", " Chapter 7 Open Questions/Issues Does it travel to Landsat 7 or even 8? Does it travel geographically to other areas? built in to create a tiff again in general built pipeline to assess if part of road exists or not. CNN? "]
]
